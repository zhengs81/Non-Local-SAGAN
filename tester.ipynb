{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0896e07-8a7c-4dfc-bcc4-93adea6741a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from sagan_models import Generator, Discriminator\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0727bb0-80b7-4ca0-a957-6e07ca99af1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from parameter import *\n",
    "from trainer import Trainer\n",
    "from data_loader import Data_Loader\n",
    "from torch.backends import cudnn\n",
    "from utils import make_folder\n",
    "import torch_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcef391-1d1c-4b5c-8b31-861298878d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0855df41-29c0-489c-b3a0-f85fb15926e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.batch_size=64\n",
    "config.imsize=64\n",
    "config.train=False\n",
    "config.pretrained_model=996975\n",
    "config.dataset=\"celeb\"\n",
    "config.version=\"sagan_celeb\"\n",
    "config.adv_loss='hinge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c318b2a-f0d4-4b6b-8e21-4331249b2ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_loader = Data_Loader(config.train, config.dataset, config.image_path, config.imsize,\n",
    "#                          config.batch_size, shuf=config.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06336d6-4a55-49ee-a35a-bc3754755e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_iter=iter(data_loader.loader())\n",
    "# for i in data_iter:\n",
    "#     print(len(i))\n",
    "#     print(i[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feda5246-3218-414b-9cd6-8ab159bda85b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(data_loader.loader(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82317ab-3de7-49df-b1e8-d44ed6c958f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da216513-f867-4e48-a178-69ba58c73d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(trainer.G,(128,1))\n",
    "# summary(trainer.D,(3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00fd298-0ee3-462a-a9eb-525920661d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    def __init__(self, config):\n",
    "        # exact model and loss\n",
    "        self.model = config.model\n",
    "        self.adv_loss = config.adv_loss\n",
    "\n",
    "        # Model hyper-parameters\n",
    "        self.imsize = config.imsize\n",
    "        self.g_num = config.g_num\n",
    "        self.z_dim = config.z_dim\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.parallel = config.parallel\n",
    "\n",
    "        self.lambda_gp = config.lambda_gp\n",
    "        self.total_step = config.total_step\n",
    "        self.d_iters = config.d_iters\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_workers = config.num_workers\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.lr_decay = config.lr_decay\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.pretrained_model = config.pretrained_model\n",
    "\n",
    "        self.dataset = config.dataset\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.image_path = config.image_path\n",
    "        self.log_path = config.log_path\n",
    "        self.model_save_path = config.model_save_path\n",
    "        self.sample_path = config.sample_path\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.model_save_step = config.model_save_step\n",
    "        self.version = config.version\n",
    "\n",
    "        # Path\n",
    "        self.log_path = os.path.join(config.log_path, self.version)\n",
    "        self.sample_path = os.path.join(config.sample_path, self.version)\n",
    "        self.model_save_path = os.path.join(config.model_save_path, self.version)\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            self.load_pretrained_model()\n",
    "    \n",
    "    def test(self):\n",
    "        make_folder('./','generated_dataset{}'.format(self.pretrained_model))\n",
    "        # 一共可得到1000*64=64000张照片\n",
    "        for i in range(1000):\n",
    "            # 一定要用randn,不可用rand！ 前者为正态分布，后者为均匀分布\n",
    "            rand_z=tensor2var(torch.randn(self.batch_size, self.z_dim))\n",
    "            fake_images,_,_=self.G(rand_z)\n",
    "            for j in range(self.batch_size):\n",
    "                save_image(denorm(fake_images[j]),'./generated_dataset'+str(self.pretrained_model)+'/{}_fake.png'.format(i*64+j + 1))\n",
    "            # save_image(denorm(fake_images),'./generated_dataset/tot.png')\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        self.G = Generator(self.batch_size,self.imsize, self.z_dim, self.g_conv_dim).cuda()\n",
    "        self.D = Discriminator(self.batch_size,self.imsize, self.d_conv_dim).cuda()\n",
    "        if self.parallel:\n",
    "            self.G = nn.DataParallel(self.G)\n",
    "            self.D = nn.DataParallel(self.D)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        # self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.G.parameters()), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.D.parameters()), self.d_lr, [self.beta1, self.beta2])\n",
    "\n",
    "        self.c_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_D.pth'.format(self.pretrained_model))))\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7780b9c-3d7b-41bb-975a-7aaf33efe587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pytorch_fid ./generated_dataset996975 ./data/CelebA/img_align_celeba/img_align_celeba --device cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a00f1e-3665-42f8-9389-9d77f5ed1198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FID=torch.zeros(10)\n",
    "# FID[0]=98.36766537081229\n",
    "# FID[5]=99.15743182237708\n",
    "# FID[9]=106.43966820133824\n",
    "# print(FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681f5aff-2a2c-4bba-bc2e-da946cf4337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 101280)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset101280\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "C:\\anaconda3\\Lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 98.58704324918315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 202560)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset202560\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 91.5720880145594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 300675)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset300675\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 95.13792657159144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 401955)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset401955\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 95.362589272337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 500070)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset500070\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 99.11009419309502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 601350)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset601350\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 97.2150263614325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 702630)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset702630\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 101.93908220990042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 800745)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset800745\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 104.40427941630787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 902025)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset902025\" with extensions png,jpg,jpeg\n",
      "Found 64000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 105.65341877352901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded trained models (step: 996975)..!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./generated_dataset996975\" with extensions png,jpg,jpeg\n",
      "Found 202560 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/CelebA/img_align_celeba/img_align_celeba\" with extensions png,jpg,jpeg\n",
      "Found 202599 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                                                                     \n",
      "Frechet Inception Distance: 106.53801210477513\n"
     ]
    }
   ],
   "source": [
    "ISCs=[]\n",
    "FIDs=[]\n",
    "configs=[]\n",
    "pretrained_models=[101280,202560,300675,401955,500070,601350,702630,800745,902025,996975]\n",
    "for i in range(10):\n",
    "    config.pretrained_model=pretrained_models[i]\n",
    "    \n",
    "    if not os.path.exists('./generated_dataset{}'.format(pretrained_models[i])):\n",
    "        tester=Tester(config)\n",
    "        tester.test()\n",
    "    \n",
    "    metrics_dict = torch_fidelity.calculate_metrics(\n",
    "        input1='./generated_dataset{}'.format(pretrained_models[i]),\n",
    "        input2='./data/CelebA/img_align_celeba/img_align_celeba',\n",
    "        cuda=True,\n",
    "        fid=True,\n",
    "        isc=True\n",
    "    )\n",
    "    \n",
    "    print(metrics_dict)\n",
    "    FIDs.append(metrics_dict['frechet_inception_distance'])\n",
    "    ISCs.append(metrics_dict['inception_score_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1a172d-ac79-467f-be13-72f4b02edc22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'frechet_inception_distance': 98.58704324918315},\n",
       " {'frechet_inception_distance': 91.5720880145594},\n",
       " {'frechet_inception_distance': 95.13792657159144},\n",
       " {'frechet_inception_distance': 95.362589272337},\n",
       " {'frechet_inception_distance': 99.11009419309502},\n",
       " {'frechet_inception_distance': 97.2150263614325},\n",
       " {'frechet_inception_distance': 101.93908220990042},\n",
       " {'frechet_inception_distance': 104.40427941630787},\n",
       " {'frechet_inception_distance': 105.65341877352901},\n",
       " {'frechet_inception_distance': 106.53801210477513}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32f10857-ee3b-4f50-996a-97d174b0b627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ISCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73689c76-65bf-4055-892e-cd793470f061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
